# -*- coding: utf-8 -*-
"""
–ú–æ–¥—É–ª—å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ –∏ –ø–æ–∏—Å–∫–æ–º
"""

import os
import time
import json
import pickle
import hashlib
from datetime import datetime
from tqdm.auto import tqdm

from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document

from cybersec_consultant.config import ConfigManager, INDICES_DIR, get_api_key
from cybersec_consultant.state_management import STATE

class VectorSearchManager:
    """–ö–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω—ã–º –ø–æ–∏—Å–∫–æ–º"""

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        self.config_manager = ConfigManager()
        self.embeddings = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è API –∫–ª—é—á–∞ –∏–∑ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        if not STATE.api_key:
            STATE.api_key = get_api_key()

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∏–Ω–¥–µ–∫—Å–æ–≤, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        os.makedirs(INDICES_DIR, exist_ok=True)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self._init_embeddings()

    def _init_embeddings(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        try:
            self.embeddings = OpenAIEmbeddings()
            print("‚úÖ –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {str(e)}")
            self.embeddings = None

    def create_index(self, documents, index_name="cybersec_index"):
        """
        –°–æ–∑–¥–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å FAISS –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

        Args:
            documents (list): –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
            index_name (str): –ò–º—è –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è

        Returns:
            FAISS: –°–æ–∑–¥–∞–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å
        """
        if not documents:
            print("‚ùå –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø—É—Å—Ç")
            return None

        if not self.embeddings:
            print("‚ùå –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            return None

        print(f"üîÑ –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å –∏–∑ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")

        try:
            # –ó–∞–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞
            start_time = time.time()

            # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å FAISS
            db = FAISS.from_documents(documents, self.embeddings)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å
            index_path = os.path.join(INDICES_DIR, index_name)
            os.makedirs(index_path, exist_ok=True)
            db.save_local(index_path)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
            docs_path = os.path.join(index_path, "documents.pkl")
            with open(docs_path, 'wb') as f:
                pickle.dump(documents, f)

            elapsed_time = time.time() - start_time
            print(f"‚úÖ –ò–Ω–¥–µ–∫—Å '{index_name}' —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω –∑–∞ {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥.")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            STATE.vector_db = db

            return db
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∏–Ω–¥–µ–∫—Å–∞: {str(e)}")
            return None

    def load_index(self, index_name="cybersec_index"):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å

        Args:
            index_name (str): –ò–º—è –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏

        Returns:
            FAISS: –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å
        """
        if not self.embeddings:
            print("‚ùå –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            return None

        index_path = os.path.join(INDICES_DIR, index_name)
        if not os.path.exists(index_path) or not os.path.exists(os.path.join(index_path, "index.faiss")):
            print(f"‚ùå –ò–Ω–¥–µ–∫—Å '{index_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω.")
            return None

        try:
            print(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω–¥–µ–∫—Å '{index_name}'...")
            start_time = time.time()

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω–¥–µ–∫—Å
            db = FAISS.load_local(index_path, self.embeddings, allow_dangerous_deserialization=True)

            elapsed_time = time.time() - start_time
            print(f"‚úÖ –ò–Ω–¥–µ–∫—Å '{index_name}' —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –∑–∞ {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥.")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            STATE.vector_db = db

            return db
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–Ω–¥–µ–∫—Å–∞: {str(e)}")
            return None

    def search_documents_with_score(self, query, k=3, use_cache=True):
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–¥–æ–∫—É–º–µ–Ω—Ç, –æ—Ü–µ–Ω–∫–∞).

        Args:
            query (str): –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            k (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            use_cache (bool): –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à –∏–ª–∏ –Ω–µ—Ç

        Returns:
            list: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–¥–æ–∫—É–º–µ–Ω—Ç, –æ—Ü–µ–Ω–∫–∞)
        """
        if STATE.vector_db is None:
            print("‚ùå –í–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω. –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∏–ª–∏ —Å–æ–∑–¥–∞–π—Ç–µ –∏–Ω–¥–µ–∫—Å.")
            return []

        # –ò—â–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        cached_results = STATE.get_search_from_cache(query, k) if use_cache else None
        if cached_results:
            print(f"üîÑ –ù–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}' (—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∑—è—Ç—ã –∏–∑ –∫—ç—à–∞)")
            return cached_results

        try:
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
            print(f"üîç –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'")
            start_time = time.time()

            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ —Å –æ—Ü–µ–Ω–∫–æ–π —Å—Ö–æ–¥—Å—Ç–≤–∞
            results_with_scores = STATE.vector_db.similarity_search_with_score(query, k=k)

            # –ò–∑–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
            execution_time = time.time() - start_time

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∫—ç—à
            if results_with_scores and use_cache:
                STATE.add_search_to_cache(query, k, results_with_scores)

            print(f"‚úÖ –ü–æ–∏—Å–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω –∑–∞ {execution_time:.2f} —Å–µ–∫.")
            print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(results_with_scores)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

            return results_with_scores

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}")
            return []
